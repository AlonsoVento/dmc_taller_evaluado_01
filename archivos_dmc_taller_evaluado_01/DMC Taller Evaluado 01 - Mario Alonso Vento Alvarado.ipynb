{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f227312-eebf-407c-9540-7a21407bfa81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Taller Evaluado 01\n",
    "#Ingesta y Validación de Datos en Azure Databricks\n",
    "**Objetivos:**\n",
    "Practicar y reforzar lo aprendido en las sesiones 2 y 3, aplicando técnicas de ingesta de datos, validación de esquemas, manejo de errores y registro de auditoría, utilizando un dataset de origen real o simulado.\n",
    "Al finalizar, seré capaz de: \n",
    "1. Seleccionar e implementar un mecanismo de ingesta adecuado (Auto Loader) para un dataset dado. \n",
    "2. Aplicar un esquema explícito (StructType) para controlar la estructura de los datos. \n",
    "3. Detectar y separar registros válidos e inválidos usando _rescued_data o badRecordsPath. \n",
    "4. Registrar los registros inconsistentes en una tabla Delta de auditoría usando saveAsTable(). \n",
    "5. Documentar el proceso y los hallazgos en un notebook bien estructurado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66486a4a-8da0-40d3-943d-fdce2631c37a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Escoger un dataset de origen (puede ser un archivo CSV, JSON o Parquet disponible en un volumen de Unity Catalog o cargado manualmente).\n",
    "Busqué DataSets en la página [KAGGLE](https://www.kaggle.com/) y obtuve el dataset de las transacciones de un E-Commerce correspondientes a los años 2010 y 2011. Debido a que el dataset original tenía 541,910 registros, lo ajusté aleatoriamente a solo 1,500 con fines prácticos para agilizar el procesamiento.\n",
    "El enlace del DataSet en mención es: [E-Commerce Data](https://www.kaggle.com/datasets/carrie1/ecommerce-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c4cad8a-4f62-41a9-832e-b12a3b9c435f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Diseñar y ejecutar un flujo de ingesta hacia Databricks utilizando Auto Loader según el formato y el volumen del archivo. \n",
    "### 3. Definir un esquema explícito con StructType y aplicarlo durante la carga."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ead5381-56f9-4d22-9960-67fe10eeb369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Procedo a crear el catálogo, volúmenes y directorio de carpetas.\n",
    "\n",
    "![creación de directorio.png](./creación de directorio.png \"creación de directorio.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e41f40b-2e02-4d96-bf1a-ef8e3597bc2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Defino el esquema con StrucType\n",
    "\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "compras_schema = (\n",
    "    StructType()\n",
    "    .add(\"InvoiceNo\", IntegerType(), True)\n",
    "    .add(\"StockCode\", IntegerType(), True)\n",
    "    .add(\"Description\", StringType(), True)\n",
    "    .add(\"Quantity\", IntegerType(), True)\n",
    "    .add(\"InvoiceDate\", StringType(), True)\n",
    "    .add(\"UnitPrice\", DoubleType(), True)\n",
    "    .add(\"CustomerID\", StringType(), True)\n",
    "    .add(\"Country\", StringType(), True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffd06661-389b-471b-9b77-35fc4a7b8475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Leo el archivo CSV\n",
    "\n",
    "from pyspark.sql.functions import col, to_timestamp, when, coalesce\n",
    "\n",
    "df_compras = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(compras_schema)\n",
    "    .load(\"/Volumes/dmc_taller01/default/source/input/csv/\")\n",
    "    .withColumn(\"archivo_origen\", col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "# Debido a que hay muchos formatos de fecha en el DataSet, defino una lista de formatos de fecha comunes:\n",
    "date_formats = [\n",
    "    \"M/d/yyyy H:mm\",    # Ej: 7/20/2011 11:28\n",
    "    \"M/d/yyyy H:mm:ss\", # Si hubiera segundos\n",
    "    \"M/dd/yyyy H:mm\",   # Ej: 10/30/2011 11:37\n",
    "    \"d/M/yyyy H:mm\",\n",
    "    \"dd/M/yyyy H:mm\"\n",
    "]\n",
    "\n",
    "# Realizo la conversión de la columna InvoiceDate usando coalesce y to_timestamp\n",
    "df_converted = df_compras.withColumn(\n",
    "    \"InvoiceDate\",\n",
    "    coalesce(*[to_timestamp(col(\"InvoiceDate\"), fmt) for fmt in date_formats])\n",
    ")\n",
    "\n",
    "# Verifico el resultado\n",
    "df_converted.printSchema()\n",
    "display(df_converted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4370086-13f4-4d7b-89ad-4069aaacdcff",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"InvoiceDate\":230},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754880261381}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Leo el archivo CSV\n",
    "display(df_compras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cdf00fc-c5c4-4358-8c78-3729fab1144b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Configurar un badRecordsPath y capturar información de registros inválidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d4b306-0137-4bf4-88dc-1b7dc6e06888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "compras_schema = (\n",
    "    StructType()\n",
    "    .add(\"InvoiceNo\", IntegerType(), True)\n",
    "    .add(\"StockCode\", IntegerType(), True)\n",
    "    .add(\"Description\", StringType(), True)\n",
    "    .add(\"Quantity\", IntegerType(), True)\n",
    "    .add(\"InvoiceDate\", StringType(), True)\n",
    "    .add(\"UnitPrice\", DoubleType(), True)\n",
    "    .add(\"CustomerID\", StringType(), True)\n",
    "    .add(\"Country\", StringType(), True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273efb01-159f-468d-8af9-eaed4b42dace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_compras = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .schema(compras_schema)\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_rescued_data\")\n",
    "    .option(\"badRecordsPath\", \"/Volumes/dmc_taller01/default/source/taller_evaluado_01/bad_records/\") # _rescued_data se almacena en un directorio\n",
    "    .load(\"/Volumes/dmc_taller01/default/source/taller_evaluado_01/\")\n",
    "    .withColumn(\"archivo_origen\", col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_compras = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .schema(compras_schema)\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_rescued_data\")\n",
    "    .option(\"badRecordsPath\", \"/Volumes/dmc_taller01/default/source/taller_evaluado_01/bad_records/\") \n",
    "    .load(\"/Volumes/dmc_taller01/default/source/input/csv/\")\n",
    "    .withColumn(\"archivo_origen\", col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "display(df_compras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cd1b2a0-1d24-402d-87c6-6868a77f227a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Evidencia de creación de carpeta BAD_RECORDS:**\n",
    "\n",
    "![creación archivo Bad_Records.png](./creación archivo Bad_Records.png \"creación archivo Bad_Records.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "869c4119-32ca-48e7-b01c-72da1453e834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Separar datos válidos e inválidos en tablas Delta diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d695d575-d2cf-4a44-a14c-c63e4d2d9b75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Lo realizo con TRY/EXCEPT. Considerando que la columa más relevante del DataSet es CustomerID, procedo con esa validación:\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "try:\n",
    "    df_validos = df_compras.filter(col(\"CustomerID\").isNotNull())\n",
    "    df_invalidos = df_compras.filter(col(\"CustomerID\").isNull())\n",
    "\n",
    "    df_validos.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dmc_taller01.default.compras_validadas\")\n",
    "    df_invalidos.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dmc_taller01.default.compras_con_errores\")\n",
    "\n",
    "    print(\"Escritura realizada con éxito.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la escritura: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "562df1a4-ccff-4a2a-b49c-3de09e340c90",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754538181651}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Muestro la data resultante de la tabla compras_con_errores\n",
    "\n",
    "select * from dmc_taller01.default.compras_con_errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d5e3e78-8932-443a-8c85-da5008f4f26c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Muestro la data resultante de la tabla compras_validadas\n",
    "\n",
    "select * from dmc_taller01.default.compras_validadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03258011-1b93-4a8c-990f-8f14e3122ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Evidencia de creación de Tablas Delta:**\n",
    "\n",
    "![creación tablas Delta.png](./creación tablas Delta.png \"creación tablas Delta.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b74e02c6-4f7b-4548-b67c-5fe87fceb6c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Agregar una columna de auditoría con la ruta de archivo de origen (_metadata.file_path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b671082-6406-4c57-a678-9ae7f626e006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_compras = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .schema(compras_schema)\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_rescued_data\")\n",
    "    .load(\"/Volumes/dmc_taller01/default/source/input/csv/\")\n",
    "    .withColumn(\"Auditoria\", col(\"_metadata.file_path\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08a0ccb-f2c8-4100-a826-591b6cf01740",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754886759278}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_compras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44b41444-8721-4b9f-b1f4-9ea5f4032ba3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Guardar los registros inválidos en una tabla Delta de auditoría usando saveAsTable()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d906c3-8f47-4266-a038-e2dc6b2b55a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS dmc_taller01\")\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_invalidos = df_compras.filter(col(\"CustomerID\").isNull())\n",
    "df_invalidos.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").saveAsTable(\"dmc_taller01.default.compras_con_errores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64f5f40-6018-49bf-884f-56dbfd733b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from dmc_taller01.default.compras_con_errores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5004eb73-3514-4cd2-a5c7-d23388b66a25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. Documentar las decisiones tomadas, las dificultades encontradas y los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb90de12-6710-482b-a1d0-68997711a282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Decisiones tomadas**\n",
    "a. Se definió como columna clave de validez del registro al CustomerID en caso esté vacío o no.\n",
    "b. Se determinó como Auto Loader la técnica más apta para la ingesta de la data.\n",
    "c. Se redujo la cantidad de registros a la cantidad original, con fines didácticos.\n",
    "\n",
    "**Dificultades encontradas**\n",
    "a. El reto propio de indagar respecto a la herramienta Databricks, la cual he empezado a estudiar a profundidad\n",
    "\n",
    "**Resultados Obtenidos**\n",
    "a. Principalmente, aplicar los conocimientos adquiridos en las sesiones dictadas hasta el momento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b3621fd-3625-48f6-a9dc-f7cf48ee2e41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6833279782799206,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DMC Taller Evaluado 01 - Mario Alonso Vento Alvarado",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
